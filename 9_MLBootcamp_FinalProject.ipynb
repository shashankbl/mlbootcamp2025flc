{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "11FFJD9ABN7y5Y02cn2fko_ET6FhmI6AF",
      "authorship_tag": "ABX9TyNRwf5QOSg+8b4QwarINn8o",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shashankbl/mlbootcamp2025flc/blob/main/9_MLBootcamp_FinalProject.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Machine Learning Bootcamp 2025\n",
        "\n",
        "### Final Project: Train a Deep Learning model to identify Grocery item"
      ],
      "metadata": {
        "id": "DoJf9K5qna65"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms, models\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import os\n",
        "import csv\n",
        "from datetime import datetime"
      ],
      "metadata": {
        "id": "W9Dh8fuk22i9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# NOTE: Create directory 'Datasets/GroceryStoreDataset', unzip the shared dataset in it and mount the Google Drive\n",
        "# The original dataset used is: https://www.kaggle.com/datasets/validmodel/grocery-store-dataset?resource=download and it has been reduced further for our use-case\n",
        "data_dir = '/content/drive/MyDrive/Datasets/GroceryStoreDataset'  # Mount dataset in Google Drive"
      ],
      "metadata": {
        "id": "i9aVO01L27cu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Device: {device}\")"
      ],
      "metadata": {
        "id": "DiJakegpcKuW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_class_names(csv_file_path):\n",
        "    \"\"\"\n",
        "    Extracts class names from a CSV file and returns them as a list.\n",
        "\n",
        "    Args:\n",
        "        csv_file_path (str): The path to the CSV file.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing a list of class names and the number of classes.\n",
        "               Returns (None, 0) if the file does not exist or an error occurs.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        with open(csv_file_path, 'r') as file:\n",
        "            reader = csv.reader(file)\n",
        "            next(reader)  # Skip header row if it exists\n",
        "            class_names = [row[2] for row in reader]  # Assuming class names are in the first column\n",
        "            class_names = sorted(set(class_names))\n",
        "        return class_names, len(class_names)\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: File '{csv_file_path}' not found.\")\n",
        "        return None, 0\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "        return None, 0"
      ],
      "metadata": {
        "id": "0czv7aeM3wTU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data augmentation and normalization\n",
        "data_transforms = {\n",
        "    'train': transforms.Compose([\n",
        "        transforms.RandomResizedCrop(224),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "    'test': transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "}"
      ],
      "metadata": {
        "id": "u8SYV_DN4HLk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Identify that all folders are accessible from mounted Google Drive\n",
        "for x in ['train', 'test']:\n",
        "  path_new = os.path.join(data_dir, x)\n",
        "  print(path_new)\n",
        "  for folder in os.listdir(path_new):\n",
        "    folder_path = os.path.join(path_new, folder)\n",
        "    print(folder_path)\n"
      ],
      "metadata": {
        "id": "bzWojEfRI_mG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x])\n",
        "                  for x in ['train', 'test']}  # Assuming you have train and test folders\n",
        "dataloaders = {x: DataLoader(image_datasets[x], batch_size=16, shuffle=True, num_workers=2)\n",
        "              for x in ['train', 'test']}\n",
        "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'test']}\n",
        "\n",
        "print(f\"Dataset sizes: {dataset_sizes}\")"
      ],
      "metadata": {
        "id": "QxlSKZia4Nnq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_names = image_datasets['train'].classes\n",
        "\n",
        "print(class_names)\n",
        "print(len(class_names))\n",
        "\n",
        "class_names = image_datasets['test'].classes\n",
        "\n",
        "print(class_names)\n",
        "print(len(class_names))"
      ],
      "metadata": {
        "id": "dQfGgMDJJzbS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load pre-trained EfficientNetB4\n",
        "model = models.efficientnet_b4(pretrained=True)\n",
        "\n",
        "# Modify the classifier\n",
        "num_ftrs = model.classifier[1].in_features\n",
        "model.classifier[1] = nn.Linear(num_ftrs, num_classes)\n",
        "model = model.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)"
      ],
      "metadata": {
        "id": "-HTLL6AAcz85"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_image(image_path, model, class_names):\n",
        "  img = Image.open(image_path).convert('RGB')\n",
        "  img_t = data_transforms['test'](img).unsqueeze(0)\n",
        "  img_t = img_t.to(device)\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    out = model(img_t)\n",
        "    _, index = torch.max(out, 1)\n",
        "    percentage = torch.nn.functional.softmax(out, dim=1)[0] * 100\n",
        "    print(f\"Predicted Class: {class_names[index[0]]}, Confidence: {percentage[index[0]].item():.2f}%\")"
      ],
      "metadata": {
        "id": "xiQz7UkGmp-T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, criterion, optimizer, num_epochs=25):\n",
        "    history = {'train_loss': [], 'train_acc': [], 'test_loss': [], 'test_acc': []}\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
        "        now = datetime.now()\n",
        "        print(now.strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
        "        print('-' * 10)\n",
        "\n",
        "        for phase in ['train', 'test']:\n",
        "            if phase == 'train':\n",
        "                model.train()\n",
        "            else:\n",
        "                model.eval()\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "            i = 0\n",
        "            for inputs, labels in dataloaders[phase]:\n",
        "                i += 1\n",
        "                if i % 10 == 0:\n",
        "                    print(f\"Batch {i} of {len(dataloaders[phase])}\")\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "                optimizer.zero_grad()\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = model(inputs)\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "                    loss = criterion(outputs, labels)\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "            epoch_loss = running_loss / dataset_sizes[phase]\n",
        "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
        "\n",
        "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
        "\n",
        "            history[f'{phase}_loss'].append(epoch_loss)\n",
        "            history[f'{phase}_acc'].append(epoch_acc)\n",
        "\n",
        "    return model, history"
      ],
      "metadata": {
        "id": "uYHty5B3c0tR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Before training\n",
        "predict_image('/content/drive/MyDrive/Datasets/GroceryStoreDataset/test/Mango/Mango_002.jpg', model, class_names)\n",
        "predict_image('/content/drive/MyDrive/Datasets/GroceryStoreDataset/test/Pineapple/Pineapple_021.jpg', model, class_names)\n"
      ],
      "metadata": {
        "id": "_JS0IIf3nomp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model, history = train_model(model, criterion, optimizer, num_epochs=18)\n"
      ],
      "metadata": {
        "id": "XmK6M7JfdI6F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metrics = {\n",
        "    'train_loss': history['train_loss'],\n",
        "    'train_acc': [t.cpu().item() for t in history['train_acc']],\n",
        "    'test_loss': history['test_loss'],\n",
        "    'test_acc': [t.cpu().item() for t in history['test_acc']]\n",
        "}\n",
        "\n",
        "print(metrics)\n",
        "\n",
        "# Plot the training history\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(metrics['train_loss'], label='Train Loss')\n",
        "plt.plot(metrics['test_loss'], label='Test Loss')\n",
        "plt.legend()\n",
        "plt.title('Loss')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(metrics['train_acc'], label='Train Accuracy')\n",
        "plt.plot(metrics['test_acc'], label='Test Accuracy')\n",
        "plt.legend()\n",
        "plt.title('Accuracy')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "t16NKwwMk8Ll"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save the trained model\n",
        "torch.save(model, '/content/drive/MyDrive/Datasets/GroceryStoreDataset/complete_model.pth')\n"
      ],
      "metadata": {
        "id": "UIVE84gMkWdD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# After training\n",
        "predict_image('/content/drive/MyDrive/Datasets/GroceryStoreDataset/test/Mango/Mango_002.jpg', model, class_names)\n",
        "predict_image('/content/drive/MyDrive/Datasets/GroceryStoreDataset/test/Pineapple/Pineapple_021.jpg', model, class_names)\n"
      ],
      "metadata": {
        "id": "dK1rXYLzdRf_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Project:\n",
        "\n",
        "1. Perform exploratory data analysis on the 'train' and 'test' datasets to calculate class imbalance (by comparing 'samples per class' across all the classes)\n",
        "2. Print confusion matrix, precision, recall and f1-score\n",
        "3. Show a grid of 6x4 images, with actual and predicted class for each of those\n",
        "\n",
        "\n",
        "### Bonus Project:\n",
        "\n",
        "1. Allow user to input the items they shopped using images, use model to identify grocery item based on confidence threshold. If confidence is low, ask user to manually input the item.\n",
        "2. Update the digital grocery cart\n",
        "3. Process the transaction by generating a transaction receipt\n"
      ],
      "metadata": {
        "id": "LTcyENYCtxXm"
      }
    }
  ]
}